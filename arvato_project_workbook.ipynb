{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Create a Customer Segmentation Report for Arvato Financial Services\n",
    "\n",
    "In this project, you will analyze demographics data for customers of a mail-order sales company in Germany, comparing it against demographics information for the general population. You'll use unsupervised learning techniques to perform customer segmentation, identifying the parts of the population that best describe the core customer base of the company. Then, you'll apply what you've learned on a third dataset with demographics information for targets of a marketing campaign for the company, and use a model to predict which individuals are most likely to convert into becoming customers for the company. The data that you will use has been provided by our partners at Bertelsmann Arvato Analytics, and represents a real-life data science task.\n",
    "\n",
    "If you completed the first term of this program, you will be familiar with the first part of this project, from the unsupervised learning project. The versions of those two datasets used in this project will include many more features and has not been pre-cleaned. You are also free to choose whatever approach you'd like to analyzing the data rather than follow pre-determined steps. In your work on this project, make sure that you carefully document your steps and decisions, since your main deliverable for this project will be a blog post reporting your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Get to Know the Data\n",
    "\n",
    "There are four data files associated with this project:\n",
    "\n",
    "- `Udacity_AZDIAS_052018.csv`: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns).\n",
    "- `Udacity_CUSTOMERS_052018.csv`: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns).\n",
    "- `Udacity_MAILOUT_052018_TRAIN.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns).\n",
    "- `Udacity_MAILOUT_052018_TEST.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns).\n",
    "\n",
    "Each row of the demographics files represents a single person, but also includes information outside of individuals, including information about their household, building, and neighborhood. Use the information from the first two files to figure out how customers (\"CUSTOMERS\") are similar to or differ from the general population at large (\"AZDIAS\"), then use your analysis to make predictions on the other two files (\"MAILOUT\"), predicting which recipients are most likely to become a customer for the mail-order company.\n",
    "\n",
    "The \"CUSTOMERS\" file contains three extra columns ('CUSTOMER_GROUP', 'ONLINE_PURCHASE', and 'PRODUCT_GROUP'), which provide broad information about the customers depicted in the file. The original \"MAILOUT\" file included one additional column, \"RESPONSE\", which indicated whether or not each recipient became a customer of the company. For the \"TRAIN\" subset, this column has been retained, but in the \"TEST\" subset it has been removed; it is against that withheld column that your final predictions will be assessed in the Kaggle competition.\n",
    "\n",
    "Otherwise, all of the remaining columns are the same between the three data files. For more information about the columns depicted in the files, you can refer to two Excel spreadsheets provided in the workspace. [One of them](./DIAS Information Levels - Attributes 2017.xlsx) is a top-level list of attributes and descriptions, organized by informational category. [The other](./DIAS Attributes - Values 2017.xlsx) is a detailed mapping of data values for each feature in alphabetical order.\n",
    "\n",
    "In the below cell, we've provided some initial code to load in the first two datasets. Note for all of the `.csv` data files in this project that they're semicolon (`;`) delimited, so an additional argument in the [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) call has been included to read in the data properly. Also, considering the size of the datasets, it may take some time for them to load completely.\n",
    "\n",
    "You'll notice when the data is loaded in that a warning message will immediately pop up. Before you really start digging into the modeling and analysis, you're going to need to perform some cleaning. Take some time to browse the structure of the data and look over the informational spreadsheets to understand the data values. Make some decisions on which features to keep, which features to drop, and if any revisions need to be made on data formats. It'll be a good idea to create a function with pre-processing steps, since you'll need to clean all of the datasets before you work with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUSINESS UNDERSTANDING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marketing is crucial for the growth and sustainability of the business as it helps build companyâ€™s brand, engage customers, grow revenues and increase sales. One of the key painpoint of business is to understand customers and identify their needs in order to tailor campaigns to customer segments most likely to purchase products.\n",
    "Customer segmentation  helps business plan marketing campaigns easier, focusing on certain customer groups instead of targeting the mass market, therefore more efficient in terms of time, money and other resources. \n",
    "* What are the relationship beween demographics of the company's existing customers and the general population of Germany?\n",
    "* Which parts of the general population that are more likely to be part of the mail-order company's main customer bases, and which parts of the general population are less so\n",
    "* How historical demographic data can help business to build prediction model, therefore be able to identify potential customers.<br> \n",
    "\n",
    "Fortunately, those business questions can be solved using analytics by involving appropriate data analytics tools and methodologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA UNDERSTANDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# load in population data\n",
    "azdias = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_AZDIAS_052018.csv', sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in customers data\n",
    "customers = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_CUSTOMERS_052018.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding column descriptions and domain values\n",
    "\n",
    "* load Attributes and Values files\n",
    "* merge those 2 files to create a data dictionary for Arvato files\n",
    "* create a list of tuples which contains column name and its 'unknown' domain values\n",
    "* create a generic function to display the % of missing values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Attributes file and fill in missing values for column Information Level\n",
    "attributes =  pd.read_excel('DIAS Information Levels - Attributes 2017.xlsx',sheet_name=None)\n",
    "attb_desc = list(attributes.values())[0]\n",
    "attb_desc['Information level'] = attb_desc['Information level'].fillna(method='ffill')\n",
    "attb_desc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('attribute:  {}  description:  {}'.format(attb_desc['Attribute'],attb_desc['Description']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Values file and filling missing values for column Attribute\n",
    "values =  pd.read_excel('DIAS Attributes - Values 2017.xlsx', sheet_name=None)\n",
    "attb_vals = list(values.values())[0]\n",
    "attb_vals['Attribute'] = attb_vals['Attribute'].fillna(method='ffill')\n",
    "attb_vals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# join Attributes and Values files to form data dictionary for Arvato files\n",
    "\n",
    "data_dictionary = pd.merge(attb_vals, attb_desc, on='Attribute')\n",
    "data_dictionary = data_dictionary[['Information level','Attribute','Description_x','Value','Meaning','Additional notes']]\n",
    "data_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dictionary[data_dictionary['Attribute']=='CJT_GESAMTTYP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display columns and associate values meaning 'unknown'\n",
    "\n",
    "NaN_meanings = ['unknown','no classification possible','unknown / no main age detectable','no transaction known']\n",
    "NaN_df = data_dictionary[data_dictionary.Meaning.isin(NaN_meanings)]\n",
    "NaN_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaN_df['Value'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of tuples which contains column name and associate 'unknown' values\n",
    "idx = NaN_df.index\n",
    "unknown_list =[]\n",
    "for i in idx:\n",
    "    val = NaN_df['Value'][i]\n",
    "    if val == '-1, 0' or val == '-1, 9':\n",
    "        tupl = (NaN_df['Attribute'][i],val[0:2])\n",
    "        unknown_list.append(tupl)\n",
    "        tupl = (NaN_df['Attribute'][i],val[4:6])\n",
    "        unknown_list.append(tupl)\n",
    "    else:\n",
    "        tupl = (NaN_df['Attribute'][i],str(val))\n",
    "        unknown_list.append(tupl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unknown_value(col,value):\n",
    "    '''\n",
    "    This functions check if a value in a column match 'unknown' definition is data dictionary\n",
    "    INPUT: - a column in a dataset\n",
    "           - a value in the column\n",
    "    OUTPUT:\n",
    "           return true if the value of the column match 'unknown' definition in data dictionary, otherwise return false\n",
    "    '''\n",
    "    result = bool()\n",
    "    tup = (col,value)\n",
    "    if tup in unknown_list:\n",
    "        result = True\n",
    "    else:\n",
    "        result = False\n",
    "\n",
    "    return result\n",
    "\n",
    "def percent_missing_values(df):\n",
    "    '''\n",
    "    This function calculate and display % of missing values in each column of dataset\n",
    "    INPUT: pandas dataframe\n",
    "    OUTPUT: dictionary with key is the column name and value is % of missing\n",
    "    '''\n",
    "    missing_dict = {}\n",
    "    total_rec = df.shape[0]\n",
    "    \n",
    "    for col in df.columns.values:  #iterate columns\n",
    "        # if column contain NaN values, calculate the mean of missing values\n",
    "        if df[col].isnull().values.any():  \n",
    "            missing_dict[col]= df[col].isnull().mean()\n",
    "        # if column does not contain NaN values, map values to 'unknown' value in data dictionary\n",
    "        else:\n",
    "            s = df[col].value_counts(dropna=False)\n",
    "            nan_count = 0\n",
    "            for index, value in s.items():        \n",
    "                if check_unknown_value(col,str(index)) :\n",
    "                    nan_count = nan_count + value\n",
    "            missing_dict[col]= round(nan_count/total_rec,4)\n",
    "            \n",
    "    # print dictionary in reverse order (highest-> lowest % missing values)\n",
    "    sorted_d = dict( sorted(missing_dict.items(), key=operator.itemgetter(1),reverse=True))\n",
    "        \n",
    "    missing_dict = sorted_d\n",
    "    \n",
    "    return missing_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_percent_missing(df, percent):\n",
    "    '''\n",
    "    This function displays and return columns which have % missing greater than certain percentage\n",
    "    INPUT: - input pandas dataframe\n",
    "           - percent , enter 50 for 50%\n",
    "    OUTPUT: a list of columns which have % missing greater than <percent>\n",
    "    '''\n",
    "    cols = []\n",
    "    \n",
    "    missing_dict = percent_missing_values(df)\n",
    "    for key,value in missing_dict.items():\n",
    "        if value > percent/100:\n",
    "            cols.append(key)\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis - Population file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display top5 rows in population file\n",
    "azdias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "azdias.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display % of missing in population file\n",
    "missing_pop = percent_missing_values(azdias)\n",
    "for key,val in missing_pop.items():\n",
    "    print('{} - {}'.format(key,val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the columns which have % missing greater than 30%\n",
    "cols_pop = top_percent_missing(azdias, 30)\n",
    "cols_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_hist_plot(missing_dict):\n",
    "    '''\n",
    "    '''\n",
    "    missing_list = []\n",
    "    for value in missing_dict.values():\n",
    "        missing_list.append(math.ceil(value*100))\n",
    "    \n",
    "    return missing_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis - Customer file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display top5 rows in customers file\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers['PRODUCT_GROUP'].value_counts()/customers.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers['CUSTOMER_GROUP'].value_counts()/customers.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers['ONLINE_PURCHASE'].value_counts()/customers.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = list(customers.select_dtypes(exclude=['int64','float64']).columns)\n",
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = list(customers.select_dtypes(include=['int64','float64']).columns)\n",
    "len(numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display % of missing in customers file\n",
    "missing_cust = percent_missing_values(customers)\n",
    "for key,val in missing_cust.items():\n",
    "    print('{} - {}'.format(key,val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the columns which have % missing greater than 50%\n",
    "cols_cust = top_percent_missing(customers, 50)\n",
    "cols_cust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare % of missing data between Population and Customer files\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, sharey=True,figsize=(12,7))\n",
    "ax1.set(xlabel = '% missing values', title = 'GENERAL POPULATION')\n",
    "ax2.set(xlabel = '% missing values', title = 'CUSTOMERS')\n",
    "sns.countplot(prepare_hist_plot(missing_pop), ax=ax1, palette ='husl')\n",
    "sns.countplot(prepare_hist_plot(missing_cust), ax=ax2, palette ='husl')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('popluation total records:' ,azdias.shape[0])\n",
    "print('customer total records:' ,customers.shape[0])\n",
    "azdias.shape[0]/customers.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis - Summary\n",
    "\n",
    "* The domain values represent 'unknown' are not consistent between columns. Different values meaning the same thing (ie. 0, -1,9, NaN)\n",
    "* Volumn of general population file is nearly 5 times larger than that of customer file. This implies 20% of population is Arvato's customers\n",
    "* Distribution of missing values are slightly difference between General Population and Customer fiels.Around 80+ columns have 0% missing values.Most columns have percentage missing values between 10% and 30%. The MODE % of missing values in Population file is 12%, whereas in Customer file is 27%\n",
    "* Many columns have identical % of missing values, it is likely that those colums are relevant.\n",
    "* Only 10% of customers purchase online, 30% are single buyers vs 70% multi buyers\n",
    "* 8 variables are categorical, the rest are numeric variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPARATION\n",
    "\n",
    "\n",
    "Create generic function to prepare data for machine learning, which includes:\n",
    "* drop rows with more than 75% missing values\n",
    "* drop columns with more than 70% missing values\n",
    "* drop customer id column \n",
    "* drop categorical columns\n",
    "* drop 3 columns exist in Customer file but not exist in Population file\n",
    "* for numeric variables, replace NaN with values implying 'unknown' in data dictionary, in this case is -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df,percent):\n",
    "    '''\n",
    "    This function performs below:\n",
    "    - drop columns with more than input percent\n",
    "    - drop customer ID column\n",
    "    - drop categorical columns\n",
    "    INPUT: input pandas dataframe\n",
    "    OUTPUT: pandas dataframe after columns being removed\n",
    "    '''\n",
    "    # drop columns with more than percent missing values\n",
    "    #missing_dict = percent_missing_values(df)\n",
    "    cols = top_percent_missing(df, percent)\n",
    "    missing_df = df.drop(columns= cols)\n",
    "    \n",
    "    # drop customer ID column\n",
    "    missing_df.drop(['LNR'], axis=1,inplace=True)\n",
    "    \n",
    "    # drop categorical columns\n",
    "    categorical_cols = list(df.select_dtypes(exclude=['int64','float64']).columns)\n",
    "    dropped_df = missing_df.drop(columns= categorical_cols)\n",
    "      \n",
    "    \n",
    "    return dropped_df\n",
    "\n",
    "def drop_rows(df,value_threshold):\n",
    "    '''\n",
    "    This function drops records with number of Nan greater than threshold\n",
    "    INPUT: - input pandas dataframe , \n",
    "           - threshold of missing values in a row\n",
    "    OUTPUT: pandas dataframe after rows being removed\n",
    "    '''\n",
    "    # drop rows which have more than threshold missing values\n",
    "    nan_rows =  df.isnull().sum(axis=1)\n",
    "    droped_rows = list(nan_rows[nan_rows.values >=value_threshold].index)\n",
    "    dropped_df = df.drop(droped_rows)\n",
    "    \n",
    "    return dropped_df\n",
    "\n",
    "def prepare_data(df,percent_threshold,value_threshold = 0):\n",
    "    '''\n",
    "    This function peforms below:\n",
    "    - drop rows with number of missing values greater than threshold\n",
    "    - drop columns not useful for machine learning\n",
    "    - fill NaN values with -1\n",
    "\n",
    "    INPUT: \n",
    "    - input pandas dataframe, \n",
    "    - acceptable percentage of missing values (columns with % > threshold will be removed )\n",
    "    OUTPUT: cleaned pandas dataframe\n",
    "    '''\n",
    "    \n",
    "    # drop rows \n",
    "    if value_threshold == 0:  #threshold not supplied, \n",
    "        row_df = df  # no row dropped\n",
    "    else:\n",
    "        row_df = drop_rows(df,value_threshold)\n",
    "    \n",
    "    # drop columns \n",
    "    col_df = drop_columns(row_df,percent_threshold)  \n",
    "\n",
    "    \n",
    "    # fill NaN with -1 values\n",
    "    clean_df = col_df.fillna(-1)\n",
    "    \n",
    "   \n",
    "    return clean_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check no of missing values in each rows in population file\n",
    "nan_rows_pop = azdias.shape[1] - azdias.count(axis=1)\n",
    "nan_rows_pop.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean population file, remove columns with more than 70% Nan and rows whose NaN counts beyond 3rd ISQ (75%)\n",
    "pop_cleaned_df = prepare_data(azdias,70,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check no of missing values in each rows in customers file\n",
    "nan_rows_cust = customers.shape[1] - customers.count(axis=1)\n",
    "nan_rows_cust.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean customers file, remove columns with more than 70% Nan and rows whose NaN counts beyond 3rd ISQ (75%)\n",
    "cust_cleaned_df = prepare_data(customers,70,225)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "def scale_numeric_var(df):\n",
    "    ''' This function scales numeric variables in df dataset\n",
    "    INPUT:  pandas dataset\n",
    "    OUTPUT: scaled dataset\n",
    "    '''\n",
    "\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df),\n",
    "                              index=df.index, columns=df.columns)\n",
    "    \n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale population data\n",
    "df_scaled_pop = scale_numeric_var(pop_cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled_pop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale customer data\n",
    "df_scaled_cust = scale_numeric_var(cust_cleaned_df)\n",
    "df_scaled_cust.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca_data = pca.fit(df_scaled_pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw PCA chart\n",
    "\n",
    "num_components= len(pca.explained_variance_ratio_)\n",
    "idx = np.arange(num_components)\n",
    "ratio = pca.explained_variance_ratio_\n",
    " \n",
    "plt.figure(figsize=(13, 9))\n",
    "ax = plt.subplot(111)\n",
    "cumvals = np.cumsum(ratio)\n",
    "ax.bar(idx, ratio)\n",
    "ax.plot(idx, cumvals)\n",
    "for i in range(num_components):\n",
    "    if(i%20 == 0 or i<6):\n",
    "        ax.annotate(r\"%s%%\" % ((str(ratio[i]*100)[:4])), (idx[i]+0.2, ratio[i]), va=\"bottom\", ha=\"center\", fontsize=9)\n",
    " \n",
    "    ax.xaxis.set_tick_params(width=0, gridOn=True)\n",
    "    ax.yaxis.set_tick_params(width=2, length=10, gridOn=True)\n",
    " \n",
    "ax.set_xlabel(\"Principal Components\")\n",
    "ax.set_ylabel(\"% Variance Explained\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate PCA with n =150 and apply to population \n",
    "pca = PCA(n_components=150)\n",
    "pca_pop = pca.fit_transform(df_scaled_pop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 10 records for the first Principle Component\n",
    "pca_map = pd.DataFrame({'weight': pca.components_[0],'name': df_scaled_pop.columns})        \n",
    "pca_map = pca_map.sort_values(by='weight', ascending=False)\n",
    "pca_map.iloc[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 10 records for the second Principle Component\n",
    "pca_map = pd.DataFrame({'weight': pca.components_[1],'name': df_scaled_pop.columns})        \n",
    "pca_map = pca_map.sort_values(by='weight', ascending=False)\n",
    "pca_map.iloc[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 10 records for the third Principle Component\n",
    "pca_map = pd.DataFrame({'weight': pca.components_[2],'name': df_scaled_pop.columns})        \n",
    "pca_map = pca_map.sort_values(by='weight', ascending=False)\n",
    "pca_map.iloc[:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the optimum number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_scores = [] \n",
    "range_values =  range(1, 12)\n",
    "for i in range_values:\n",
    "  kmeans = KMeans(n_clusters = i)\n",
    "  kmeans.fit(pca_pop)\n",
    "  k_scores.append(kmeans.inertia_) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot k_scores\n",
    "plt.plot(k_scores, 'bx-')\n",
    "plt.title('Finding right number of clusters')\n",
    "plt.xlabel('Clusters')\n",
    "plt.ylabel('scores') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for k in k_scores:\n",
    "    print(k-i)\n",
    "    i = k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Kmeans method on population and customer file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply Kmeans with n=5\n",
    "kmeans = KMeans(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit clustering model to Population file\n",
    "clustering_model = kmeans.fit(pca_pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign clusters to general population and customer\n",
    "pca_cust = pca.fit_transform(df_scaled_cust)\n",
    "\n",
    "cluster_pop = clustering_model.predict(pca_pop)\n",
    "cluster_cust = clustering_model.predict(pca_cust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUMMARY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Customer Segmentation Report\n",
    "\n",
    "The main bulk of your analysis will come in this part of the project. Here, you should use unsupervised learning techniques to describe the relationship between the demographics of the company's existing customers and the general population of Germany. By the end of this part, you should be able to describe parts of the general population that are more likely to be part of the mail-order company's main customer base, and which parts of the general population are less so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster_df(df):\n",
    "    '''\n",
    "    This functions create a dictionary with key is cluster label and value contains % of customers in each cluster\n",
    "    INPUT: cluster array\n",
    "    OUTPUT: dictionary contains % of customers in each cluster\n",
    "    '''\n",
    "    total_records = len(df)\n",
    "    cluster_dict = {}\n",
    "    \n",
    "    unique, counts = np.unique(df, return_counts=True)\n",
    "\n",
    "    d = dict(zip(unique, counts))   \n",
    "    for index, value in d.items():\n",
    "        cluster_dict[index+1] = round(value/total_records,5)\n",
    "        \n",
    "    return cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot customers and general population clusters\n",
    "\n",
    "pop_clusters = create_cluster_df(cluster_pop)\n",
    "cust_clusters = create_cluster_df(cluster_cust)\n",
    "plt.rcParams[\"figure.figsize\"] = (15,8)\n",
    "#plt.figure(figsize=(40,30)) \n",
    "fig, (ax1, ax2) = plt.subplots(1,2, sharey=True)\n",
    "\n",
    "ax1.set_title('General Population Clusters')\n",
    "ax2.set_title('Customer Clusters')\n",
    "sns.barplot(x=list(pop_clusters.keys()), y=list(pop_clusters.values()), ax=ax1)\n",
    "sns.barplot(x=list(cust_clusters.keys()), y=list(cust_clusters.values()), ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CLUSTER DISTRIBUTION - GENERAL POPULATION vs CUSTOMERS\")\n",
    "print('-------------------------------------------------------')\n",
    "for i in range(1,6):\n",
    "    print(\"Cluster: {} -  Population: {} - Customer: {}\".format(i,pop_clusters[i],cust_clusters[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Supervised Learning Model\n",
    "\n",
    "Now that you've found which parts of the population are more likely to be customers of the mail-order company, it's time to build a prediction model. Each of the rows in the \"MAILOUT\" data files represents an individual that was targeted for a mailout campaign. Ideally, we should be able to use the demographic information from each individual to decide whether or not it will be worth it to include that person in the campaign.\n",
    "\n",
    "The \"MAILOUT\" data has been split into two approximately equal parts, each with almost 43 000 data rows. In this part, you can verify your model with the \"TRAIN\" partition, which includes a column, \"RESPONSE\", that states whether or not a person became a customer of the company following the campaign. In the next part, you'll need to create predictions on the \"TEST\" partition, where the \"RESPONSE\" column has been withheld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_train = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TRAIN.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_test = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TEST.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display response distribution\n",
    "\n",
    "response = mailout_train['RESPONSE']\n",
    "unique, counts = np.unique(response, return_counts=True)\n",
    "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
    "\n",
    "ax = sns.countplot(x=response.index, data=response)\n",
    "\n",
    "ax.set_title('Response Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(response, return_counts=True)\n",
    "print('Response: {}  Count: {}'.format(unique,counts))\n",
    "print('Response: {}  Percent: {}'.format(unique,counts/len(mailout_train['RESPONSE'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate response1:response0 ratio\n",
    "response_0 = len(response) / (2 * counts[0])\n",
    "response_1 = len(response) / (2 * counts[1])\n",
    "weights = {0:response_0,1:response_1}  # this will be used as a parameter of the model creation\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this step remove columns with more than 70% NaN and fill NaN\n",
    "train_df = prepare_data(mailout_train,70)\n",
    "test_df = prepare_data(mailout_test,70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = train_df['RESPONSE']\n",
    "train_Y[0:5]\n",
    "unique, counts = np.unique(train_Y, return_counts=True)\n",
    "print(unique,counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['RESPONSE'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split train df to  train_X, train_Y\n",
    "train_X = scale_numeric_var(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale test data\n",
    "test_X = scale_numeric_var(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model using RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_evaluation = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_RFC = RandomForestClassifier(class_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model_RFC.fit(train_X, train_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rfc = model_RFC.predict(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture AUC Score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "dict_evaluation[\"Random Forest Classification\"] = roc_auc_score(train_Y,y_pred_rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model_LR = LogisticRegression(solver='lbfgs', class_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LR.fit(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr = model_LR.predict(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture AUC Score\n",
    "dict_evaluation[\"Logistis Regression\"] = roc_auc_score(train_Y,y_pred_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model using Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model_svc = SVC( class_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time model_svc.fit(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time y_pred_svc = model_svc.predict(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture AUC Score\n",
    "dict_evaluation[\"Support Vector Classification\"] = roc_auc_score(train_Y,y_pred_svc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model using Decision Tree Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model_dtc = DecisionTreeClassifier(random_state=0,class_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time model_dtc.fit(train_X,train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time y_pred_dtc = model_dtc.predict(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture AUC Score\n",
    "dict_evaluation[\"Decision Tree Classification\"] = roc_auc_score(train_Y,y_pred_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 algorithms have been used to build model for mailout_train. The one with highest AUC score will be deployed on mailout_test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the report\n",
    "print('---------- MODEL EVALUATION -------------')\n",
    "for key,val in dict_evaluation.items():\n",
    "    print('Model: {} - AUC Score: {} '.format(key,val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:blue\">Model Evaluation Summary:</span><br><br>\n",
    "From the model evaluation results above, I can see 'Support Vector Classification' and 'Decision Tree Classification have the same highest score. So I compare their runtimes to determine which one is WINNER<br><br>\n",
    "Support Vector Classifiction:<br>\n",
    "    * Fit:  CPU times: user 14min 9s, sys: 715 ms, total: 14min 10s \n",
    "    * Predict:  user 43.1 ms, sys: 39.9 ms, total: 83 ms\n",
    "Decision Tree Classification:<br>\n",
    "    * Fit:   CPU times: user 4.64 s, sys: 47.9 ms, total: 4.69 s\n",
    "    * Predict:  CPU times: user 43.1 ms, sys: 39.9 ms, total: 83 ms\n",
    "Support Vector Classification run 168 times longer than Decision Tree Classification (14mins vs 5 secs) <br>\n",
    "Obviously, DECISION TREE CLASSIFICATION is winner, therefore will be deployed\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL DEPLOYMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict response for mailout_test\n",
    "response_pred  = model_dtc.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict response probability for mailout_test\n",
    "response_probas = model_dtc.predict_proba(test_X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge model prediction results with mailout_test\n",
    "predictions = pd.Series(data=response_pred, index=test_X.index, name='predicted_response')\n",
    "probabilities = pd.DataFrame(data=response_probas, index=test_X.index, columns=['prob_0','RESPONSE'])\n",
    "\n",
    "results_test = mailout_test.join(predictions, how='left')\n",
    "results_test = results_test.join(probabilities, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Kaggle Competition\n",
    "\n",
    "Now that you've created a model to predict which individuals are most likely to respond to a mailout campaign, it's time to test that model in competition through Kaggle. If you click on the link [here](http://www.kaggle.com/t/21e6d45d4c574c7fa2d868f0e8c83140), you'll be taken to the competition page where, if you have a Kaggle account, you can enter. If you're one of the top performers, you may have the chance to be contacted by a hiring manager from Arvato or Bertelsmann for an interview!\n",
    "\n",
    "Your entry to the competition should be a CSV file with two columns. The first column should be a copy of \"LNR\", which acts as an ID number for each individual in the \"TEST\" partition. The second column, \"RESPONSE\", should be some measure of how likely each individual became a customer â€“ this might not be a straightforward probability. As you should have found in Part 2, there is a large output class imbalance, where most individuals did not respond to the mailout. Thus, predicting individual classes and using accuracy does not seem to be an appropriate performance evaluation method. Instead, the competition will be using AUC to evaluate performance. The exact values of the \"RESPONSE\" column do not matter as much: only that the higher values try to capture as many of the actual customers as possible, early in the ROC curve sweep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARE CSV FILE FOR KAGGLE COMPETITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test['RESPONSE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_response = results_test[['LNR','RESPONSE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_response.set_index('LNR', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data in the file before submission\n",
    "print('Check file dimension: ',mailout_response.shape)\n",
    "print('Check the random 10 rows:', mailout_response[58:68])\n",
    "unique, counts = np.unique(mailout_response, return_counts=True)\n",
    "print('Check counts in each probability class')\n",
    "for unique,counts in zip(unique,counts):\n",
    "    print('probability: {} - count: {} '.format(unique,counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CSV file for Kaggle Competition\n",
    "mailout_response.to_csv('avrvato_response.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final check to ensure the codes are error free after executing the whole workbook\n",
    "print(\"Congratulations! arvato_project_workbook.ipynb execution completed successfully \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
